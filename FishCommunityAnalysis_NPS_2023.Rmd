---
title: "FishCommunityAnalysis_NPS_2023"
author: "MPeipoch"
date: "2023-12-03"
output: pdf_document
---

Necessary packages for the script
```{r, echo=FALSE}
# required packages
req_packs <- c("ggplot2", "dplyr", "tidyr","stringr","openxlsx","sf","raster","trend")


########### if installed, then install and load ###################################
new_packs <- req_packs[!(req_packs %in% installed.packages()[,"Package"])]
if (length(new_packs) > 0) {
  install.packages(new_packs,dependencies=T)
}

lapply(req_packs, function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    stop(paste("Package", pkg, "could not be loaded. Fix it!"))
  }
  library(pkg, character.only = TRUE)
})




```


This first section is the same Dan uses. This will facilitate furture dual analsysis of water quality and fish community.
# Extract from NCRN EDD dataset
# Dan Myers, 11/27/2023


```{r}
################################################################################
### Format data ################################################################
################################################################################

# Set working directory
setwd("G:/My Drive/NPS_FishDataAnalysis/")

# Read readme
# readLines("readme.txt")

# Read EDD data
fileName <- "20231128_wqp_wqx_bss_wq_npsncrn" # Leave out .csv extension
wdata <- read.csv(paste(fileName,".csv",sep=""))

# Format dates as date
wdata$ActivityStartDate <- as.Date(wdata$ActivityStartDate)

# Save character results for later
wdata$char_results <- wdata$ResultMeasureValue

# Format result as numeric
wdata$ResultMeasureValue <- as.numeric(wdata$ResultMeasureValue)

# Charlie's string parsing script to create comment fields
st_visit_regex <- '(?<=Station_Visit_Comment":").*?(?=",)'
act_regex <- '(?<=Activity_Comment":").*?(?=",)'
samp_regex <- '(?<=Sampleability":").*?(?=",)'
flow_regex <- '(?<=Flow_Status_Choice_List":").*?(?=")'
wdata <- wdata %>% dplyr::mutate(
  Station_Visit_Comment = stringr::str_extract(ActivityCommentText, st_visit_regex)
  ,Activity_Comment = stringr::str_extract(ActivityCommentText, act_regex)
  ,Sampleability = stringr::str_extract(ActivityCommentText, samp_regex)
  ,Flow_Status_Choice_List = stringr::str_extract(ActivityCommentText, flow_regex)
)

################################################################################
### Extract WQ data ############################################################
################################################################################

# Select WQ data by project identifier
wqdata <- wdata[wdata$ProjectIdentifier=="USNPS NCRN Perennial stream water monitoring",] # 

# Choose WQ columns
wqdata <- wqdata[c("ActivityStartDate", "MonitoringLocationIdentifier", "CharacteristicName",
                   "ResultMeasureValue", "MeasureQualifierCode", "ResultValueTypeName",
                   "LaboratoryName", "ResultCommentText")]

# Take median of multiple measurements across stream
wdata_avgd <- wqdata %>%
  group_by(MonitoringLocationIdentifier) %>%
  group_by(ActivityStartDate, .add=T) %>%
  group_by(CharacteristicName, .add=T) %>%
  summarise(ResultMeasureValue = median(ResultMeasureValue, na.rm=T)) # Changed from mean to median

################################################################################
### Extract biological data ####################################################
################################################################################

# Select biological data by project identifier
biodata <- wdata[wdata$ProjectIdentifier=="USNPS NCRN Biological stream survey",] # 

# Choose Biological columns
biodata_narrow <- biodata[c("ActivityStartDate", "MonitoringLocationIdentifier", "CharacteristicName",
                   "ResultMeasureValue", "MeasureQualifierCode", "ResultValueTypeName",
                   "SubjectTaxonomicName", "ResultCommentText")]

################################################################################
### Explore biological data ####################################################
################################################################################

# Find biological characteristics
unique(biodata$CharacteristicName)

# Find biological sites
unique(biodata$MonitoringLocationIdentifier)

# Find biological site stream names
unique(biodata$MonitoringLocationName)

# Find biological sample dates
unique(biodata$ActivityStartDate)

# Check out result identifiers (may not be very useful)
head(biodata$ResultIdentifier)

# Check out site visit names
head(biodata$ActivityMediaSubdivisionName)

# Check out result units
unique(biodata$ResultMeasure.MeasureUnitCode)

# Latitude and longitude (numeric)
head(biodata$ActivityLocation.LatitudeMeasure)
head(biodata$ActivityLocation.LongitudeMeasure)

# Taxon
unique(biodata$SubjectTaxonomicName)[1:5]

# Activity comments
unique(biodata$ActivityCommentText)[1:5]

# Result comments
unique(biodata$ResultCommentText)[1:5]

# Sample comments (electrofishing pass number)
unique(biodata$SampleCollectionMethod.MethodDescriptionText)


################################################################################
### Filter biological data #####################################################
################################################################################

# Example 1: Find fish lengths for 2022-07-25 and NCRN_NACE_OXRU
biodata_narrow %>% filter(MonitoringLocationIdentifier %in% c("NCRN_NACE_OXRU") &
                  ActivityStartDate=="2022-07-25" &
                  CharacteristicName %in% c("fish - individual fish total length"))

# Example 2: Find the average fish mass etc. for each taxon
biodata_narrow %>% filter(CharacteristicName=="fish - individual fish mass") %>%
  group_by(MonitoringLocationIdentifier) %>%
  group_by(SubjectTaxonomicName, .add=T) %>%
  summarise(Min=summary(ResultMeasureValue)[1],
            Q1=summary(ResultMeasureValue)[2],
            Median=summary(ResultMeasureValue)[3],
            Mean=summary(ResultMeasureValue)[4],
            Q3=summary(ResultMeasureValue)[5],
            Max=summary(ResultMeasureValue)[6],
            NAs=summary(ResultMeasureValue)[7],
            n=n(),
            SD=sd(ResultMeasureValue,na.rm=T))


################################################################################
### Some plots of WQ against watershed conditions ##############################
################################################################################

################################################################################
### Load watershed conditions data #############################################

# Source functions
source("Functions.R")

# Remove sites not currently monitored
current_sites <- wdata_avgd %>%
  group_by(MonitoringLocationIdentifier) %>%
  summarise(start_date = min(ActivityStartDate, na.rm=T),
            end_date = max(ActivityStartDate, na.rm=T))
current_sites <- current_sites[year(current_sites$end_date)==2023,]
wdata_avgd <- wdata_avgd[wdata_avgd$MonitoringLocationIdentifier %in% current_sites$MonitoringLocationIdentifier,]

# Load watershed conditions summaries
shed_sums <- read.csv("Watershed conditions 2023_09_19.csv") %>%
  rename(MonitoringLocationIdentifier=NEW_IMLOCI)

# Load EPA modeled background SC from Olson & Cormier
back_sc <- read.csv("NCRN_Monitoring_Locations_with_background_SC_2023_09_15.csv")

# Remove extra sites (Make sure the field and site names are up-to-date)
back_sc$MonitoringLocationIdentifier <- back_sc$IMLOCID
back_sc <- back_sc[back_sc$MonitoringLocationIdentifier %in% current_sites$MonitoringLocationIdentifier,]
back_sc <- back_sc[!duplicated(back_sc$IMLOCID),]
shed_sums <- shed_sums[shed_sums$MonitoringLocationIdentifier %in% current_sites$MonitoringLocationIdentifier,]

# Load NLCD land cover summaries for each watershed
lc_chg <- read.csv("watersheds_nlcd06-19_change_2023_09_15.csv") %>% na.omit()
sheds_nlcd06 <- read.csv("watersheds_nlcd06_stats_2023_09_15.csv") %>% na.omit()
sheds_nlcd19 <- read.csv("watersheds_nlcd19_stats_2023_09_15.csv") %>% na.omit()

# Load watersheds shapefile
sheds <- st_read("NCRN_watersheds_2023_07_06.shp")

# Load road salt data
salt_all <- read.csv("Watershed road salt annual.csv")

# Remove inactive sites
salt <- salt_all[salt_all$NEW_IMLOCI %in% current_sites$MonitoringLocationIdentifier,]

# Calculate summaries
CharacteristicName <- "Specific conductance"
outputs <- medians_func(current_sites, wdata_avgd, CharacteristicName, back_sc, shed_sums)


################################################################################
### Make plots of land cover vs. specific conductance ##########################
ylabs <- "Specific cond. (uS/cm)"
ylim <- 1600
threshold <- 171
plotting_func(wdata_avgd, outputs, ylabs,ylim, threshold, CharacteristicName)


################################################################################
### Make plots of water quality and urbanization trends ########################

# Summarise by site and year
wq_annual <- wdata_avgd[wdata_avgd$CharacteristicName==CharacteristicName,] %>%
  group_by(CharacteristicName, .add=FALSE) %>%
  group_by(MonitoringLocationIdentifier, .add=TRUE) %>%
  group_by(year(ActivityStartDate), .add=TRUE) %>%
  summarise(annual_avg = median(as.numeric(ResultMeasureValue), na.rm=T), 
            n=sum(as.numeric(ResultMeasureValue)>-1000, na.rm=T)) %>%
  rename(year_wq=`year(ActivityStartDate)`) %>%
  arrange(MonitoringLocationIdentifier, year_wq)

# Calculate Sen's slopes
sens_slopes <- wq_annual %>%
  group_by(MonitoringLocationIdentifier) %>%
  summarise(sen_slope = sens.slope(annual_avg)$estimates,
            z = sens.slope(annual_avg)$statistic,
            mk_p = sens.slope(annual_avg)$p.value,
            n=sens.slope(annual_avg)$parameter)

# Calculate total urban change (expansion)
urb_df <- data.frame(MonitoringLocationIdentifier=lc_chg$X, urb_chg=rowSums(lc_chg[,4:7]))

# Calculate urban intensification (infill)
inf <- data.frame(MonitoringLocationIdentifier=as.numeric(lc_chg$X))
for (i in 1:nrow(lc_chg[,4:7])){
  for (j in 1:ncol(lc_chg[,4:7])){
    if ((lc_chg[,4:7]>0)[i,j]){
      inf[i,j] <- lc_chg[,4:7][i,j]
    }
  }
}
urb_df$urb_inf <- rowSums(inf[],na.rm=T)

# Calculate combined urbanization rate (expansion + infill)
urb_df$urb_com <- urb_df$urb_chg + urb_df$urb_inf

# Calculate total urban (avg of years)
urb_df$urb_tot <- rowSums((sheds_nlcd06[,4:7]+sheds_nlcd19[,4:7])/2)

# Make data frame
urb_df[,2:5] <- round(urb_df[,2:5],3)
urb_df <- urb_df[urb_df$MonitoringLocationIdentifier %in% current_sites$MonitoringLocationIdentifier,]

# Join LULC with WQ
spc_join <- left_join(x=sens_slopes, y=urb_df, by="MonitoringLocationIdentifier")

# Plot it
trends_func(spc_join)


################################################################################
### Proportion of streams with median SC above the critical value, by park. ####
data.frame(name=substr(outputs$MonitoringLocationIdentifier,6,9), 
           medians=outputs$medians) %>%
  group_by(name) %>%
  summarise(pct_above = (sum(medians>threshold) / length(medians)*100))

################################################################################
### Analyze salinity ###########################################################

# Extract salinity
CharacteristicName2 <- "Salinity"

# Calculate summaries
outputs2 <- medians_func(current_sites, wdata_avgd, CharacteristicName2, back_sc, shed_sums)

# Plot salinity
ylabs2 <- "Salinity (ppt)"
ylim2 <- 12
threshold2 <- 0.5
boxplot_func(wdata_avgd, outputs2, ylabs2,ylim2, threshold2, CharacteristicName2)


################################################################################
### Road salt analysis##########################################################

# Turn-off scientific notation
options(scipen=999)  

### Comparison plots
# Plot it
windows(6.5, 6.5)
par(mar=c(6,4,1,1), mgp=c(2,1,0), mfrow=c(2,1))

# Set up data
box_data_raw <- t(salt[,17:31]) *0.453592 # Convert lbs to kg
colnames(box_data_raw) <- substr(current_sites$MonitoringLocationIdentifier,6,14)

# Order data
meds <- rep(NA,length(current_sites$MonitoringLocationIdentifier))
for (j in 1:length(meds)){
  meds[j] <- median(box_data_raw[,j], na.rm=T) 
}
box_data <- box_data_raw[,order(meds)]

# Make box plot
boxplot(box_data/10000, las=2, ylab="salt x10^5 kg / km2 / yr", cex.axis=0.75)
grid()
boxplot(box_data/10000, las=2, add=T, cex.axis=0.75)
title("a)",adj=0.01, line=-1.2, cex.main=1.5)


### Scatterplots with WQ data
# Calculate 2005-2019 avg
avg_salt <- data.frame(MonitoringLocationIdentifier=substr(current_sites$MonitoringLocationIdentifier,1,14), avg_lbs = rowMeans(salt[,17:31]), area_km = salt$Area_km)
avg_salt$avg_kg = avg_salt$avg*0.453592

# Join data
plot_data <- left_join(avg_salt,outputs, by="MonitoringLocationIdentifier")
plot_data$avg_kg <- plot_data$avg_kg / 10000

# Remove unneeded site and order
plot_data <- plot_data[plot_data$MonitoringLocationIdentifier != "NCRN_GWMP_SPRU",]
plot_data <- plot_data[order(plot_data$medians),]

# Make plot
par(mar=c(3,4,0,1))
plot(plot_data$avg_kg, plot_data$medians, xlab="salt x10^5 kg / km2 / yr", 
     ylab = "Sp. Cond. (uS/cm)", type="n",ylim=c(0,max(plot_data$q90th, na.rm=T)))
grid()
points(plot_data$avg_kg, plot_data$q1, col="blue")
points(plot_data$avg_kg, plot_data$medians, col="orange")
points(plot_data$avg_kg, plot_data$q3, col="red")
points(plot_data$avg_kg, plot_data$q90th, col="darkred")

# Add models
y = plot_data$q1
x = plot_data$avg_kg
model <- lm(y ~ x+I(x^2))
myPredict <- predict( model ) 
ix <- sort(x,index.return=T)$ix
lines(x[ix], myPredict[ix], col="blue", lwd=1 )  
summary(model)

y = plot_data$medians
x = plot_data$avg_kg
model <- lm(y ~ x+I(x^2))
myPredict <- predict( model ) 
ix <- sort(x,index.return=T)$ix
lines(x[ix], myPredict[ix], col="orange", lwd=1 )  
summary(model)

y = plot_data$q3
x = plot_data$avg_kg
model <- lm(y ~ x+I(x^2))
myPredict <- predict( model ) 
ix <- sort(x,index.return=T)$ix
lines(x[ix], myPredict[ix], col="red", lwd=1 )  
summary(model)

y = plot_data$q90th
x = plot_data$avg_kg
model <- lm(y ~ x+I(x^2))
myPredict <- predict( model ) 
ix <- sort(x,index.return=T)$ix
lines(x[ix], myPredict[ix], col="darkred", lwd=1 )  
summary(model)

# Add legend
legend("bottomright", legend=c("Extreme (Q90)", "Third quartile", "Median", "First quartile"),
       col=c("darkred","red","orange","blue"), pch=1, bg="white", cex=0.8)
title("b)",adj=0.01, line=-1.2, cex.main=1.5)


################################################################################
### Plot SC time series for each site ##########################################

# Make plots
sc_series_func(wdata_avgd[wdata_avgd$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], current_sites[current_sites$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], 1, CharacteristicName)
sc_series_func(wdata_avgd[wdata_avgd$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], current_sites[current_sites$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], 2, CharacteristicName)
sc_series_func(wdata_avgd[wdata_avgd$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], current_sites[current_sites$MonitoringLocationIdentifier!="NCRN_GWMP_SPRU",], 3, CharacteristicName)



################################################################################
### Output watershed conditions table###########################################

# Extract current sites
shed_sums_current <- shed_sums[shed_sums$MonitoringLocationIdentifier %in% current_sites$MonitoringLocationIdentifier,]

# Create watershed conditions data frame
wat <- data.frame(
  Site_name = substr(shed_sums_current$MonitoringLocationIdentifier,6,14),
  Trees = round(shed_sums_current$Watershed.Percent.Forest..DW.,0),
  Crops = round(shed_sums_current$Watershed.Percent.Agriculture..DW.,0),
  Built = round(shed_sums_current$Watershed.Percent.Urban..DW.,0),
  Protect. = round(shed_sums_current$Prot_area_pct..PAD.,0),
  Area = round(shed_sums_current$Area_km,1)) %>%
  arrange(Site_name)

# Write to csv
# write.csv(wat, "Watershed conditions table 1.csv", row.names=F)
```


```{r}

#import data
wdata = read.csv("/Users/mpeipoch/Library/CloudStorage/GoogleDrive-mpeipoch@stroudcenter.org/My Drive/NPS_FishDataAnalysis/20231128_wqp_wqx_bss_wq_npsncrn_trainning.csv")
  
  
# Get unique sites in biological data of the factor variable
sites = unique(wdata$MonitoringLocationIdentifier)

# Results' list
diversity_metrics <- list()

# Loop through each unique site
for (site in sites) {
  
  # Subset the data for the current site
  subset_data = wdata %>%
    
    dplyr::filter(MonitoringLocationIdentifier == "NCRN_CATO_BLBZ" ) %>% #select site
    
           dplyr::select(MonitoringLocationIdentifier,
           ActivityStartDate,
           CharacteristicName,
           ResultMeasureValue) %>% #clean columns
              
                #species counts and summarize
                dplyr::filter(CharacteristicName == "specimen common name" ) %>% 
                group_by(ActivityStartDate,ResultMeasureValue) %>%
                dplyr::summarise(count = n())

  
  
  # Store the result in the list
  diversity_metrics[[as.character(site)]] <- 

}

  
  


```
























